{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hK5cflzA0OFN"
      },
      "outputs": [],
      "source": [
        "# @title Load the Drive helper and mount\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Imports\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, words\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import spacy\n",
        "from spacy.lang.ro.examples import sentences\n",
        "import unicodedata\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "!python -m spacy download ro_core_news_sm"
      ],
      "metadata": {
        "id": "GAeN8Rcd1yx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title We load the dataset\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab/Facultate/Master/Anul II/NLP2/cleaned_dataset.csv\")\n",
        "\n",
        "reviews = [df.iloc[i]['review'] for i in range(len(df.index))]\n",
        "labels = [int(df.iloc[i]['label']) for i in range(len(df.index))]"
      ],
      "metadata": {
        "id": "66p0aNfP16Qt"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Data preprocessing\n",
        "\n",
        "def preprocess_review(review):\n",
        "    # Normalize the text font\n",
        "    review = unicodedata.normalize('NFKC', review)\n",
        "    # Replace inconsistent characters\n",
        "    review = review.replace('“', '\"').replace('”', '\"')\n",
        "    review = review.replace('ş', 'ș').replace(\"Ş\", \"Ș\").replace('ţ', 'ț').replace(\"Ţ\", \"Ț\")\n",
        "    # Remove links\n",
        "    review = re.sub(r'https?://\\S+', '', review)\n",
        "    # Remove included english reviews\n",
        "    rev_beginning = [\"english review:\", \"english:\", \"[english]\"]\n",
        "    indices = [review.lower().find(b) for b in rev_beginning]\n",
        "    if indices[0] != -1:\n",
        "        index = indices[0]\n",
        "    elif indices[1] != -1:\n",
        "        index = indices[1]\n",
        "    elif indices[2] != -1:\n",
        "        index = indices[2]\n",
        "    else:\n",
        "        index = -1\n",
        "    review = review[:index]\n",
        "\n",
        "    return review\n",
        "\n",
        "reviews = [preprocess_review(review) for review in reviews]"
      ],
      "metadata": {
        "id": "KKLnrGWh21om"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title We see on average how lengthy is each review type, based on the number of characters\n",
        "\n",
        "len_reviews = [len(reviews[i]) for i in range(len(reviews))]\n",
        "len_reviews_0 = [len(reviews[i]) for i in range(len(reviews)) if labels[i] == 0]\n",
        "len_reviews_1 = [len(reviews[i]) for i in range(len(reviews)) if labels[i] == 1]\n",
        "len_reviews_2 = [len(reviews[i]) for i in range(len(reviews)) if labels[i] == 2]\n",
        "\n",
        "np.mean(len_reviews_0), np.mean(len_reviews_1), np.mean(len_reviews_2)"
      ],
      "metadata": {
        "id": "gsoMl9KRR91c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title We visualize the samples after data preprocessing\n",
        "\n",
        "def plot_samples(X, y, mode, mode_str):\n",
        "    plt_colors = ['tab:orange', 'tab:purple', 'tab:green']\n",
        "\n",
        "    transformer = mode(n_components=2)\n",
        "    X_transformed = transformer.fit_transform(X)\n",
        "\n",
        "    plt.rc('axes', axisbelow=True)\n",
        "    plt.grid()\n",
        "\n",
        "    positive = plt.scatter([X_transformed[i][0] for i in range(len(X_transformed)) if y[i] == 2], [X_transformed[i][1] for i in range(len(X_transformed)) if y[i] == 2], marker='.', color=plt_colors[2])\n",
        "    neutral = plt.scatter([X_transformed[i][0] for i in range(len(X_transformed)) if y[i] == 1], [X_transformed[i][1] for i in range(len(X_transformed)) if y[i] == 1], marker='.', color=plt_colors[1])\n",
        "    negative = plt.scatter([X_transformed[i][0] for i in range(len(X_transformed)) if y[i] == 0], [X_transformed[i][1] for i in range(len(X_transformed)) if y[i] == 0], marker='.', color=plt_colors[0])\n",
        "\n",
        "    plt.legend((negative, neutral, positive),\n",
        "               ('Negative', 'Neutral', 'Positive'),\n",
        "               scatterpoints=1,\n",
        "               loc='best',\n",
        "               ncol=4,\n",
        "               fontsize=8)\n",
        "\n",
        "    plt.title(f\"{mode_str} analysis of samples\")\n",
        "    plt.xlabel(\"x coord\")\n",
        "    plt.ylabel(\"y coord\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Convert text to embeddings\n",
        "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2') # Pre-trained model\n",
        "embeddings = model.encode(reviews)\n",
        "\n",
        "plot_samples(embeddings, labels, PCA, 'PCA')"
      ],
      "metadata": {
        "id": "E7B0xnkO1-MB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Negative and positive words distribution in each review type"
      ],
      "metadata": {
        "id": "RdSi5W4RCGX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We first get the lists of possible negative and positive words and remove the stop words from them as they are not relevant\n",
        "\n",
        "stop_words = set(stopwords.words('romanian'))\n",
        "\n",
        "negative_words = []\n",
        "with open(\"/content/drive/MyDrive/Colab/Facultate/Master/Anul II/NLP2/negative_words_ro.txt\", \"r\") as f:\n",
        "    for line in f:\n",
        "        if line.strip() not in stop_words:\n",
        "            negative_words.append(line.strip())\n",
        "\n",
        "positive_words = []\n",
        "with open(\"/content/drive/MyDrive/Colab/Facultate/Master/Anul II/NLP2/positive_words_ro.txt\", \"r\") as f:\n",
        "    for line in f:\n",
        "        if line.strip() not in stop_words:\n",
        "            positive_words.append(line.strip())"
      ],
      "metadata": {
        "id": "cktZv3Yu3N5K"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We then get the number of all negative and positive words from each review type\n",
        "\n",
        "# Function to count positive and negative words\n",
        "def count_words(text, positive_words, negative_words):\n",
        "    positive_count = 0\n",
        "    negative_count = 0\n",
        "    words = text.lower().split()\n",
        "    for word in words:\n",
        "        if word in positive_words:\n",
        "            positive_count += 1\n",
        "        elif word in negative_words:\n",
        "            negative_count += 1\n",
        "    return positive_count, negative_count\n",
        "\n",
        "# Create lists to store word counts for each review type\n",
        "positive_counts, negative_counts = [], []\n",
        "review_types = [\"Negative reviews\", \"Neutral reviews\", \"Positive reviews\"]\n",
        "\n",
        "for label in range(3): # Iterate over each label (0, 1, 2)\n",
        "  positive_counts_label, negative_counts_label = [], []\n",
        "  for i in range(len(reviews)):\n",
        "      if labels[i] == label:\n",
        "          positive, negative = count_words(reviews[i], positive_words, negative_words)\n",
        "          positive_counts_label.append(positive)\n",
        "          negative_counts_label.append(negative)\n",
        "  positive_counts.append(positive_counts_label)\n",
        "  negative_counts.append(negative_counts_label)\n",
        "\n",
        "# Obtain the total positive and negative words in each review type\n",
        "total_positive_counts = [sum(counts) for counts in positive_counts]\n",
        "total_negative_counts = [sum(counts) for counts in negative_counts]"
      ],
      "metadata": {
        "id": "7DfaKof99XSq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the stacked bar chart\n",
        "\n",
        "width = 0.5\n",
        "x = range(3) # Create x-coordinates for the bar chart positions\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.grid(zorder=0)\n",
        "# Plot stacked bars for each label\n",
        "ax.bar(x, total_positive_counts, width, label='Positive words')\n",
        "ax.bar(x, total_negative_counts, width, bottom=total_positive_counts, label='Negative words')\n",
        "\n",
        "ax.set_xlabel('Review type')\n",
        "ax.set_ylabel('Word count')\n",
        "ax.set_title('Distribution of sentiment words in each type of review')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(review_types)\n",
        "ax.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WWY5kPY69Xn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PoS analysis"
      ],
      "metadata": {
        "id": "yBzBQPkHMCGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all PoS from each review\n",
        "\n",
        "spacy.require_gpu()\n",
        "\n",
        "nlp = spacy.load(\"ro_core_news_sm\")\n",
        "\n",
        "d_neg, d_neu, d_pos = {}, {}, {}\n",
        "\n",
        "for i in range(len(reviews)):\n",
        "    doc = nlp(reviews[i])\n",
        "    for token in doc:\n",
        "        if labels[i] == 0:\n",
        "            if token.pos_ not in d_neg:\n",
        "                d_neg[token.pos_] = 1\n",
        "            else:\n",
        "                d_neg[token.pos_] += 1\n",
        "        elif labels[i] == 1:\n",
        "            if token.pos_ not in d_neu:\n",
        "                d_neu[token.pos_] = 1\n",
        "            else:\n",
        "                d_neu[token.pos_] += 1\n",
        "        else:\n",
        "            if token.pos_ not in d_pos:\n",
        "                d_pos[token.pos_] = 1\n",
        "            else:\n",
        "                d_pos[token.pos_] += 1\n",
        "\n",
        "# Sort the PoS from each review type in descending order by their number of appearances\n",
        "d_neg = dict(sorted(d_neg.items(), key=lambda item: item[1], reverse=True))\n",
        "d_neu = dict(sorted(d_neu.items(), key=lambda item: item[1], reverse=True))\n",
        "d_pos = dict(sorted(d_pos.items(), key=lambda item: item[1], reverse=True))"
      ],
      "metadata": {
        "id": "FvsiuRcUFgrv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Intra-review type analysis\n",
        "\n",
        "noun_adv_neg, noun_verb_neg, noun_adj_neg, adv_verb_neg, adv_adj_neg, verb_adj_neg = d_neg['NOUN'] / d_neg['ADV'], d_neg['NOUN'] / d_neg['VERB'], d_neg['NOUN'] / d_neg['ADJ'], d_neg['ADV'] / d_neg['VERB'], d_neg['ADV'] / d_neg['ADJ'], d_neg['VERB'] / d_neg['ADJ']\n",
        "noun_adv_neu, noun_verb_neu, noun_adj_neu, adv_verb_neu, adv_adj_neu, verb_adj_neu = d_neu['NOUN'] / d_neu['ADV'], d_neu['NOUN'] / d_neu['VERB'], d_neu['NOUN'] / d_neu['ADJ'], d_neu['ADV'] / d_neu['VERB'], d_neu['ADV'] / d_neu['ADJ'], d_neu['VERB'] / d_neu['ADJ']\n",
        "noun_adv_pos, noun_verb_pos, noun_adj_pos, adv_verb_pos, adv_adj_pos, verb_adj_pos = d_pos['NOUN'] / d_pos['ADV'], d_pos['NOUN'] / d_pos['VERB'], d_pos['NOUN'] / d_pos['ADJ'], d_pos['ADV'] / d_pos['VERB'], d_pos['ADV'] / d_pos['ADJ'], d_pos['VERB'] / d_pos['ADJ']\n",
        "\n",
        "print(noun_adv_neg / noun_adv_neu, noun_verb_neg / noun_verb_neu, noun_adj_neg / noun_adj_neu, adv_verb_neg / adv_verb_neu, adv_adj_neg / adv_adj_neu, verb_adj_neg / verb_adj_neu)\n",
        "print(noun_adv_neg / noun_adv_pos, noun_verb_neg / noun_verb_pos, noun_adj_neg / noun_adj_pos, adv_verb_neg / adv_verb_pos, adv_adj_neg / adv_adj_pos, verb_adj_neg / verb_adj_pos)\n",
        "print(noun_adv_neu / noun_adv_pos, noun_verb_neu / noun_verb_pos, noun_adj_neu / noun_adj_pos, adv_verb_neu / adv_verb_pos, adv_adj_neu / adv_adj_pos, verb_adj_neu / verb_adj_pos)"
      ],
      "metadata": {
        "id": "dzGnAQyLFPt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inter-review type analysis\n",
        "\n",
        "print(d_neg['NOUN'] / d_neu['NOUN'])\n",
        "print(d_neg['AUX'] / d_neu['AUX'])\n",
        "print(d_neg['ADP'] / d_neu['ADP'])\n",
        "print(d_neg['PRON'] / d_neu['PRON'])\n",
        "print(d_neg['ADV'] / d_neu['ADV'])\n",
        "print(d_neg['DET'] / d_neu['DET'])\n",
        "print(d_neg['VERB'] / d_neu['VERB'])\n",
        "print(d_neg['ADJ'] / d_neu['ADJ'])\n",
        "print(d_neg['CCONJ'] / d_neu['CCONJ'])\n",
        "print(d_neg['SCONJ'] / d_neu['SCONJ'])\n",
        "print(d_neg['NUM'] / d_neu['NUM'])\n",
        "\n",
        "print()\n",
        "\n",
        "print(d_neg['NOUN'] / d_pos['NOUN'])\n",
        "print(d_neg['AUX'] / d_pos['AUX'])\n",
        "print(d_neg['ADP'] / d_pos['ADP'])\n",
        "print(d_neg['PRON'] / d_pos['PRON'])\n",
        "print(d_neg['ADV'] / d_pos['ADV'])\n",
        "print(d_neg['DET'] / d_pos['DET'])\n",
        "print(d_neg['VERB'] / d_pos['VERB'])\n",
        "print(d_neg['ADJ'] / d_pos['ADJ'])\n",
        "print(d_neg['CCONJ'] / d_pos['CCONJ'])\n",
        "print(d_neg['SCONJ'] / d_pos['SCONJ'])\n",
        "print(d_neg['NUM'] / d_pos['NUM'])\n",
        "\n",
        "print()\n",
        "\n",
        "print(d_neu['NOUN'] / d_pos['NOUN'])\n",
        "print(d_neu['AUX'] / d_pos['AUX'])\n",
        "print(d_neu['ADP'] / d_pos['ADP'])\n",
        "print(d_neu['PRON'] / d_pos['PRON'])\n",
        "print(d_neu['ADV'] / d_pos['ADV'])\n",
        "print(d_neu['DET'] / d_pos['DET'])\n",
        "print(d_neu['VERB'] / d_pos['VERB'])\n",
        "print(d_neu['ADJ'] / d_pos['ADJ'])\n",
        "print(d_neu['CCONJ'] / d_pos['CCONJ'])\n",
        "print(d_neu['SCONJ'] / d_pos['SCONJ'])\n",
        "print(d_neu['NUM'] / d_pos['NUM'])"
      ],
      "metadata": {
        "id": "Hh5WzjwzFQXb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
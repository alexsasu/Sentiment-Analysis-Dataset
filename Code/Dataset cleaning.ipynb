{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uE2-2ooEumqm"
      },
      "outputs": [],
      "source": [
        "# @title Load the Drive helper and mount\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Imports\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import IsolationForest"
      ],
      "metadata": {
        "id": "giKSEdIwwwLx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title We load the dataset\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab/Facultate/Master/Anul II/NLP2/dataset.csv\")"
      ],
      "metadata": {
        "id": "CE7c7v0wu6gZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cleaning the dataset\n",
        "\n",
        "def remove_outliers(data_cleaned, reviews):\n",
        "    # Convert text to embeddings\n",
        "    model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2') # Pre-trained model\n",
        "    reviews = [data_cleaned.iloc[i]['review'] for i in range(len(data_cleaned.index))]\n",
        "    embeddings = model.encode(reviews)\n",
        "\n",
        "    # Detect anomalies using Isolation Forest\n",
        "    clf = IsolationForest(random_state=42, contamination=0.05) # Set contamination (percentage of anomalies)\n",
        "    anomaly_labels = clf.fit_predict(embeddings) # -1 for anomaly, 1 for normal\n",
        "\n",
        "    # Remove anomalous reviews\n",
        "    anomaly_labels_indices = [i for i in range(len(anomaly_labels)) if anomaly_labels[i] == -1]\n",
        "    data_cleaned = data_cleaned.drop(anomaly_labels_indices)\n",
        "\n",
        "    return data_cleaned, embeddings, anomaly_labels\n",
        "\n",
        "data_cleaned = df.copy()\n",
        "\n",
        "# Step 1: Manually remove reviews that contain only a link to a YouTube video for that reivew\n",
        "data_cleaned = data_cleaned.drop([3109, 3974])\n",
        "\n",
        "# Step 2: Drop the rows with missing values in the \"review\" column\n",
        "data_cleaned = data_cleaned.dropna(subset=['review'])\n",
        "\n",
        "# Step 3: Standardize the reviews by getting rid of leading and trailing whitespaces\n",
        "data_cleaned.loc[:, 'review'] = data_cleaned['review'].str.strip()\n",
        "\n",
        "# Step 4: Drop duplicate reviews\n",
        "data_cleaned = data_cleaned.drop_duplicates(subset=['review'])\n",
        "reviews = [data_cleaned.iloc[i]['review'] for i in range(len(data_cleaned.index))]\n",
        "labels = [int(data_cleaned.iloc[i]['label']) for i in range(len(data_cleaned.index))]\n",
        "\n",
        "# Step 5: Remove outliers\n",
        "data_cleaned = data_cleaned.reset_index(drop=True)\n",
        "data_cleaned_after_outliers, embeddings, anomaly_labels = remove_outliers(data_cleaned, reviews)\n",
        "\n",
        "# Update entries ids\n",
        "new_id = 0\n",
        "for i, row in data_cleaned_after_outliers.iterrows():\n",
        "    data_cleaned_after_outliers.loc[i, 'id'] = new_id\n",
        "    new_id += 1\n",
        "data_cleaned_after_outliers = data_cleaned_after_outliers.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "gtJmfQgcvIAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title We visualize details about the cleaned dataset\n",
        "\n",
        "label_counts = data_cleaned_after_outliers['label'].value_counts()\n",
        "print(label_counts)\n",
        "print()\n",
        "genre_counts = data_cleaned_after_outliers['genre'].value_counts()\n",
        "print(genre_counts)\n",
        "print()\n",
        "print(data_cleaned_after_outliers.info())\n",
        "print()\n",
        "print(data_cleaned_after_outliers.head())"
      ],
      "metadata": {
        "id": "aH_rt2Iz3fZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Saving the cleaned dataset\n",
        "\n",
        "# data_cleaned.to_csv(\"/content/drive/MyDrive/Colab/Facultate/Master/Anul II/NLP2/cleaned_dataset.csv\", index=False)"
      ],
      "metadata": {
        "id": "KsnWnQEpve8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plotting the dataset samples during outliers removal\n",
        "\n",
        "def plot_outliers(X, y, mode, mode_str):\n",
        "    plt_colors = ['tab:orange', 'tab:purple', 'tab:green', 'tab:red']\n",
        "\n",
        "    transformer = mode(n_components=2)\n",
        "    X_transformed = transformer.fit_transform(X)\n",
        "\n",
        "    plt.rc('axes', axisbelow=True)\n",
        "    plt.grid()\n",
        "\n",
        "    positive = plt.scatter([X_transformed[i][0] for i in range(len(X_transformed)) if y[i] == 2], [X_transformed[i][1] for i in range(len(X_transformed)) if y[i] == 2], marker='.', color=plt_colors[2])\n",
        "    neutral = plt.scatter([X_transformed[i][0] for i in range(len(X_transformed)) if y[i] == 1], [X_transformed[i][1] for i in range(len(X_transformed)) if y[i] == 1], marker='.', color=plt_colors[1])\n",
        "    negative = plt.scatter([X_transformed[i][0] for i in range(len(X_transformed)) if y[i] == 0], [X_transformed[i][1] for i in range(len(X_transformed)) if y[i] == 0], marker='.', color=plt_colors[0])\n",
        "    outlier = plt.scatter([X_transformed[i][0] for i in range(len(X_transformed)) if y[i] == -1], [X_transformed[i][1] for i in range(len(X_transformed)) if y[i] == -1], marker='.', color=plt_colors[3])\n",
        "\n",
        "    plt.legend((negative, neutral, positive, outlier),\n",
        "               ('Negative', 'Neutral', 'Positive', 'Outlier'),\n",
        "               scatterpoints=1,\n",
        "               loc='best',\n",
        "               ncol=4,\n",
        "               fontsize=8)\n",
        "\n",
        "    plt.title(f\"{mode_str} analysis of samples after outliers removal\")\n",
        "    plt.xlabel(\"x coord\")\n",
        "    plt.ylabel(\"y coord\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Marking the outliers with label -1 for the plotting process\n",
        "plot_labels = labels.copy()\n",
        "for i in range(len(anomaly_labels)):\n",
        "    if anomaly_labels[i] == -1:\n",
        "        plot_labels[i] = -1\n",
        "\n",
        "plot_outliers(embeddings, plot_labels, PCA, 'PCA')"
      ],
      "metadata": {
        "id": "nPepiKjIvXz9"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}